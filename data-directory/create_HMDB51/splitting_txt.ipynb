{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2021 Taiki Miyagawa and Akinori F. Ebihara\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Valid/Test Splitting (text files)\n",
    "DATADIR/HMDB51/labelstvt/ will be made.\n",
    "\n",
    "### HMDB51 Original Naming Rules of Label Texts\n",
    "<font color=red>`glob` does not work for \"[\" or \"]\". Use \"[[]\" and \"[]]\" instead. \n",
    "    `path.replace(\"[\", \"[[\").replace(\"]\", \"[]]\").replace(\"[[\", \"[[]\")` does a good job.</font>\n",
    "\n",
    "```\n",
    "#####################################\n",
    "## Naming rules in label text file ##\n",
    "#####################################\n",
    "There are totally 153 files in this folder,\n",
    "[action]_test_split[1-3].txt  corresponding to three splits reported in the paper.\n",
    "The format of each file is\n",
    "[video_name] [id]\n",
    "The video is included in the training set if id is 1\n",
    "The video is included in the testing set if id is 2\n",
    "The video is not included for training/testing if id is 0\n",
    "There should be 70 videos with id 1 , 30 videos with id 2 in each txt file.\n",
    "\n",
    "PROPERTY                                   LABELS (ABBREVIATION)\n",
    "visible body parts                         head(h), upper body(u), full body (f), lower body(l)\n",
    "camera motion                              motion (cm), static (nm)\n",
    "number of people involved in the action    Single (np1), two (np2), three (np3)\n",
    "camera viewpoint                           Front (fr), back (ba), left(le), right(ri)\n",
    "video quality                              good (goo), medium (med), ok (bad)\n",
    "\n",
    "\n",
    "####################################\n",
    "#########    Templates   ###########\n",
    "####################################\n",
    "label file names:\n",
    "ClassName_test_split[1-3].txt\n",
    "\n",
    "video names:\n",
    "VideoName_ClassName_VisibleBodyParts_CameraMotion_NumberOfPeopleInvolvedInTheAction_CameraViewpoint_VideoQuality_Number\\\n",
    ".avi ID\n",
    "\n",
    "\n",
    "####################################\n",
    "#### Examples in class \"smile\" #####\n",
    "####################################\n",
    "my_smile_smile_h_cm_np1_fr_goo_0.avi 1\n",
    "prelinger_LetsPlay1949_smile_h_nm_np1_fr_goo_27.avi 2\n",
    "prelinger_LetsPlay1949_smile_h_nm_np1_le_goo_25.avi 2\n",
    "prelinger_LetsPlay1949_smile_u_nm_np1_fr_med_24.avi 0\n",
    "prelinger_LetsPlay1949_smile_u_nm_np1_ri_med_21.avi 2\n",
    "prelinger_they_grow_up_so_fast_1_smile_u_nm_np1_fr_med_0.avi 1\n",
    "show_your_smile_-)_smile_h_nm_np1_fr_med_0.avi 1\n",
    "showyoursmile_smile_h_nm_np1_fr_goo_0.avi 1\n",
    "smile_collection_7_smile_h_nm_np1_fr_goo_0.avi 1\n",
    "smile_collection_7_smile_h_nm_np1_fr_goo_1.avi 1\n",
    "youtube_smile_response_smile_h_nm_np1_fr_goo_0.avi 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os, shutil\n",
    "from copy import copy, deepcopy\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"Define this first. E.g., /data/t-miyagawa\"\n",
    "splitnum = 1 # Official splitting. 1, 2, or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get videodir and numf\n",
    "datadir = \"{}/HMDB51png\".format(DATADIR)\n",
    "classdir = sorted(glob(datadir + \"/*\"))\n",
    "classdir = [i + \"/\" for i in classdir]\n",
    "classnames = [i[i.rfind(\"HMDB51png/\") + 10 : -1] for i in classdir]\n",
    "videodir = {\n",
    "    k : \n",
    "    sorted(glob([v for v in classdir if v.find(\"/\" + k + \"/\") != -1][0] + \"/*\"))\n",
    "    for k in classnames}\n",
    "\n",
    "numf = dict()\n",
    "for k in classnames:\n",
    "    v1 = videodir[k]\n",
    "    v2 = [i.replace(\"[\", \"[[\").replace(\"]\", \"[]]\").replace(\"[[\", \"[[]\") for i in v1]\n",
    "    numf[k] = [len(glob(_video + \"/*.png\")) for _video in v2]\n",
    "\n",
    "# Smear the keys\n",
    "numf_concat = []\n",
    "for k in classnames:\n",
    "    v = numf[k]\n",
    "    numf_concat.extend(v)\n",
    "\n",
    "videodir_concat = []\n",
    "for k in classnames:\n",
    "    v = videodir[k]\n",
    "    videodir_concat.extend(v)\n",
    "    \n",
    "# Classwise num of frames\n",
    "numf_classwise = []\n",
    "for k in classnames:\n",
    "    v = numf[k]\n",
    "    v = sum(v)\n",
    "    numf_classwise.append(v)\n",
    "    \n",
    "# Classwise num of videos (clips)\n",
    "numv_classwise = []\n",
    "for k in classnames:\n",
    "    v = videodir[k]\n",
    "    v = len(v)\n",
    "    numv_classwise.append(v)\n",
    "    \n",
    "# Classwise num of unique videos (groups)\n",
    "numuv_classwise = []\n",
    "for k in classnames:\n",
    "    v1 = videodir[k]\n",
    "        # ['DATADIR/HMDB51png/wave/20060723sfjffbartsinger_wave_f_cm_np1_ba_med_0',\n",
    "        #  'DATADIR/HMDB51png/wave/21_wave_u_nm_np1_fr_goo_5',\n",
    "        #  'DATADIR/HMDB51png/wave/50_FIRST_DATES_wave_f_cm_np1_fr_med_0',\n",
    "        #  'DATADIR/HMDB51png/wave/50_FIRST_DATES_wave_u_cm_np1_fr_goo_30',\n",
    "        #  'DATADIR/HMDB51png/wave/50_FIRST_DATES_wave_u_cm_np1_fr_med_1',\n",
    "        #  'DATADIR/HMDB51png/wave/50_FIRST_DATES_wave_u_cm_np1_fr_med_36',\n",
    "    v2 = [i[i.rfind(\"/\")+1:] for i in v1]\n",
    "        # ['20060723sfjffbartsinger_wave_f_cm_np1_ba_med_0',\n",
    "        #  '21_wave_u_nm_np1_fr_goo_5',\n",
    "        #  '50_FIRST_DATES_wave_f_cm_np1_fr_med_0',\n",
    "        #  '50_FIRST_DATES_wave_u_cm_np1_fr_goo_30',\n",
    "        #  '50_FIRST_DATES_wave_u_cm_np1_fr_med_1',\n",
    "        #  '50_FIRST_DATES_wave_u_cm_np1_fr_med_36',\n",
    "    v3 = [i[:i.rfind(k)-1] for i in v2]\n",
    "        # ['20060723sfjffbartsinger',\n",
    "        #  '21',\n",
    "        #  '50_FIRST_DATES',\n",
    "        #  '50_FIRST_DATES',\n",
    "        #  '50_FIRST_DATES',\n",
    "        #  '50_FIRST_DATES',\n",
    "    v4 = []\n",
    "    for i in v3:\n",
    "        if not i in v4:\n",
    "            v4.append(i)\n",
    "        # ['20060723sfjffbartsinger',\n",
    "        #  '21',\n",
    "        #  '50_FIRST_DATES',\n",
    "    numuv_classwise.append(len(v4))\n",
    "    \n",
    "\n",
    "# \"\"\"\n",
    "# Returns:\n",
    "#     classnames: List. Len = Num of classes. Names of classes in alphabetical order.\n",
    "#\n",
    "#     videodir: Dict. Paths to video directories. Each values (paths) are in alphabetical order of video names.\n",
    "#     numf: Dict. Num of frames for each videos. Each values (integers) are in alphabetical order of video names.\n",
    "#\n",
    "#     numf_concat: List. Len = Num of total videos. Order is the same as `videoddir_concat`.\n",
    "#     videodir_concat: List. Len = Num of total videos. Order is the same as `numf_concat`.\n",
    "#\n",
    "#     numf_classwise: List. Len = Num of classes. The classwise numbers of frames in alphabetical order of class names.\n",
    "#     numv_classwise: List. Len = Num of classes. The classwise numbers of videos in alphabetical order of class names.\n",
    "#     numuv_classwise: List. Len = Num of classes. The classwise numbers of unique videos (groups) in alphabetical order of class names.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save [train,valid,test]list[n].txt\n",
    "Change `n` = splitnum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names\n",
    "#################################################\n",
    "orglabeldir = \"{}/HMDB51/labels\".foramt(DATADIR)\n",
    "newtrtxt = \"{}/HMDB51/labelstvt/trainlist0{}.txt\".format(DATADIR, splitnum)\n",
    "newvatxt = \"{}/HMDB51/labelstvt/validlist0{}.txt\".format(DATADIR, splitnum)\n",
    "newtetxt = \"{}/HMDB51/labelstvt/testlist0{}.txt\".format(DATADIR, splitnum)\n",
    "\n",
    "# Load label text files\n",
    "#################################################\n",
    "trclips = []\n",
    "trclips_dc = dict()\n",
    "trgroups = dict()\n",
    "trgroups_concat = []\n",
    "teclips = []\n",
    "tegroups = dict()\n",
    "tegroups_concat = []\n",
    "num_clips = 0\n",
    "for classname in classnames:\n",
    "    labeltxt = orglabeldir + \"/{}_test_split{}.txt\".format(classname, splitnum) # classwise\n",
    "    \n",
    "    with open(labeltxt) as f:\n",
    "        clips = f.readlines() # list\n",
    "        num_clips += len(clips)\n",
    "        # e.g., wave_test_split1.txt:\n",
    "        # ['20060723sfjffbartsinger_wave_f_cm_np1_ba_med_0.avi 2 \\n',\n",
    "        #  '21_wave_u_nm_np1_fr_goo_5.avi 1 \\n',\n",
    "        #  '50_FIRST_DATES_wave_f_cm_np1_fr_med_0.avi 1 \\n',\n",
    "        #  '50_FIRST_DATES_wave_u_cm_np1_fr_goo_30.avi 1 \\n',\n",
    "        #  '50_FIRST_DATES_wave_u_cm_np1_fr_med_1.avi 1 \\n',\n",
    "        #  '50_FIRST_DATES_wave_u_cm_np1_fr_med_36.avi 1 \\n', ...'... \\n']\n",
    "\n",
    "    tmp1 = [\"/\" + clip[:clip.rfind(\".avi\")] for clip in clips if clip[-3] == \"1\"]\n",
    "    trclips_dc[classname] = tmp1\n",
    "    trclips.extend(tmp1)\n",
    "        # e.g., (\"/VideoName_ClassName_MetaData\")\n",
    "        # trclips = \n",
    "        # ['/April_09_brush_hair_u_nm_np1_ba_goo_0',\n",
    "        #  '/April_09_brush_hair_u_nm_np1_ba_goo_1',\n",
    "        #  '/April_09_brush_hair_u_nm_np1_ba_goo_2',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ri_med_3',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_0',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_1',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_med_2', ...]\n",
    "    tmp2 = [\"/\" + clip[:clip.rfind(\"_{}_\".format(classname))] + \"_\" + classname for clip in clips if clip[-3] == \"1\"]\n",
    "        # \"_\" + classname is necessary, because there are some videos with the same VideoName in different classes.\n",
    "        # (There is no duplication of VideoName within each single class, though).\n",
    "    trgroups[classname] = tmp2 # There can be duplicate strings in THE list in a key.\n",
    "        # e.g,, (\"/VideoName_ClassName\")\n",
    "        # tfgroups[\"bruch_hair\"] = \n",
    "        # ['/April_09_brush_hair',\n",
    "        #  '/April_09_brush_hair',\n",
    "        #  '/April_09_brush_hair',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "        #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair', ...]\n",
    "    trgroups_concat.extend(tmp2)\n",
    "    assert len(tmp1) == len(tmp2)\n",
    "    \n",
    "    tmp3 = [\"/\" + clip[:clip.rfind(\".avi\")] for clip in clips if clip[-3] == \"2\"]\n",
    "    teclips.extend(tmp3)\n",
    "    tmp4 = [\"/\" + clip[:clip.rfind(\"_{}_\".format(classname))] + \"_\" + classname for clip in clips if clip[-3] == \"2\"]\n",
    "        # \"_\" + classname is necessary, because there are some videos with the same VideoName in different classes.\n",
    "        # (There is no duplication of VideoName within each single class, though).\n",
    "    tegroups[classname] = tmp4\n",
    "    tegroups_concat.extend(tmp4)\n",
    "    assert len(tmp3) == len(tmp4)\n",
    "    \n",
    "# Assert\n",
    "assert len(trclips) + len(teclips) < num_clips # because ID 0 is removed from train/val/test split.\n",
    "\n",
    "# Assert: no duplicated names and no contamination in tr and te\n",
    "for i, v in enumerate(trclips):\n",
    "    assert not v in teclips, \"{}, {}\".format(i, v)\n",
    "for i, v in enumerate(teclips):\n",
    "    assert not v in trclips, \"{}, {}\".format(i, v)\n",
    "for i, v in enumerate(trgroups_concat):\n",
    "    assert not v in tegroups_concat, \"{}, {}\".format(i, v)\n",
    "for i, v in enumerate(tegroups_concat):\n",
    "    assert not v in trgroups_concat, \"{}, {}\".format(i, v)\n",
    "    \n",
    "# Create index set for unique videos ...\n",
    "#################################################\n",
    "trgroupsu_idxset_of_trgroups = dict()\n",
    "for k, v in trgroups.items():\n",
    "    idxset = []\n",
    "    for cnt, itr_clip in enumerate(v):\n",
    "        if cnt == 0:\n",
    "            tmp_clipname = itr_clip\n",
    "            tmp_idx = 0\n",
    "            idxset.append(tmp_idx)\n",
    "        else:\n",
    "            if tmp_clipname != itr_clip:\n",
    "                tmp_clipname = itr_clip\n",
    "                tmp_idx += 1\n",
    "                \n",
    "            idxset.append(tmp_idx)\n",
    "            \n",
    "    assert len(idxset) == len(v)        \n",
    "    trgroupsu_idxset_of_trgroups[k] = idxset\n",
    "    \n",
    "    # e.g.,\n",
    "    # trgroupsu_idxset_of_trgroups =\n",
    "    # {'brush_hair': [0,\n",
    "    #   0,\n",
    "    #   0,\n",
    "    #   1,\n",
    "    #   1,\n",
    "    #   1,\n",
    "    #   1,\n",
    "    #   2,\n",
    "    #   2, ...,22,22,23,24,25,25], ...\n",
    "    \n",
    "# ... and extract unique-group list\n",
    "#################################################\n",
    "trgroupsu = dict()\n",
    "for k, v in deepcopy(trgroups).items():\n",
    "    _tmp = list(np.unique(v)) # no duplication\n",
    "    assert len(_tmp) == trgroupsu_idxset_of_trgroups[k][-1] + 1\n",
    "    trgroupsu[k] = list(np.unique(v)) # no duplication\n",
    "\n",
    "tegroupsu = dict()\n",
    "for k, v in deepcopy(tegroups).items():\n",
    "    tegroupsu[k] = list(np.unique(v)) # no duplication\n",
    "    # Note:\n",
    "    # trgroups =\n",
    "    # {'brush_hair': \n",
    "    #  ['/April_09_brush_hair',\n",
    "    #   '/April_09_brush_hair',\n",
    "    #   '/April_09_brush_hair',\n",
    "    #   '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "    #   '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "    #   '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "    #   '/Aussie_Brunette_Brushing_Hair_II_brush_hair', ...\n",
    "    #                      is now\n",
    "    # trgroupsu = \n",
    "    # {'brush_hair': \n",
    "    #  ['/April_09_brush_hair',\n",
    "    #   '/Aussie_Brunette_Brushing_Hair_II_brush_hair', ...\n",
    "\n",
    "# New train/valid/test split\n",
    "#################################################\n",
    "newtegroupsu = deepcopy(tegroupsu)\n",
    "newtrgroupsu = []\n",
    "newvagroupsu = []\n",
    "newtrgroupsu_idxset_of_trgroups = dict()\n",
    "newvagroupsu_idxset_of_trgroups = dict()\n",
    "\n",
    "for c, cls in enumerate(classnames):\n",
    "    _tmp = trgroupsu[cls]        \n",
    "    numva = int(len(_tmp) * 0.1) # num of validation examples in class `cls`\n",
    "    assert numva > 0\n",
    "    for v in _tmp:\n",
    "        assert not v in newtrgroupsu # check there's no duplication\n",
    "        assert not v in newvagroupsu # check there's no duplication\n",
    "\n",
    "    newtrgroupsu_idxset_of_trgroups[cls] = [i for i in trgroupsu_idxset_of_trgroups[cls] if i <= trgroupsu_idxset_of_trgroups[cls][-1] - numva]\n",
    "    newvagroupsu_idxset_of_trgroups[cls] = [i for i in trgroupsu_idxset_of_trgroups[cls] if i > trgroupsu_idxset_of_trgroups[cls][-1] - numva]\n",
    "        # e.g.,\n",
    "        # newtrgroupsu_idxset_of_trgroups =\n",
    "        # {'brush_hair': [0,\n",
    "        #   0,\n",
    "        #   0,\n",
    "        #   1,\n",
    "        #   1,\n",
    "        #   1,\n",
    "        #   1,\n",
    "        #   2,\n",
    "        #   2, ..., 22,22,23], ...\n",
    "    for i in newtrgroupsu_idxset_of_trgroups[cls]:\n",
    "        assert not i in newvagroupsu_idxset_of_trgroups # no duplication\n",
    "    for i in newvagroupsu_idxset_of_trgroups[cls]:\n",
    "        assert not i in newtrgroupsu_idxset_of_trgroups # no duplication\n",
    "        \n",
    "    _tmptr = _tmp[:- numva]\n",
    "    _tmpva = _tmp[- numva:]\n",
    "    assert len(_tmptr) == newtrgroupsu_idxset_of_trgroups[cls][-1] + 1\n",
    "    newtrgroupsu.extend(_tmptr)\n",
    "    newvagroupsu.extend(_tmpva)\n",
    "    # e.g.,\n",
    "    # newtrgroupsu = \n",
    "    # ['/April_09_brush_hair',\n",
    "    #  '/Aussie_Brunette_Brushing_Hair_II_brush_hair',\n",
    "    #  '/Blonde_being_brushed_brush_hair',\n",
    "    #  '/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair',\n",
    "    #  '/Brushing_Hair_with_Beth_brush_hair',\n",
    "    \n",
    "# Assert\n",
    "for i, v in enumerate(newvagroupsu):\n",
    "    assert not v in newtrgroupsu, \"{}, {}\".format(i, v)\n",
    "for i, v in enumerate(newtrgroupsu):\n",
    "    assert not v in newtegroupsu, \"{}, {}\".format(i, v)\n",
    "for i, v in enumerate(newtegroupsu):\n",
    "    assert not v in newvagroupsu, \"{}, {}\".format(i, v)\n",
    "    \n",
    "# Fetch clip numbers\n",
    "#################################################\n",
    "newteclips = copy(teclips)\n",
    "newtrclips = []\n",
    "newvaclips = []\n",
    "\n",
    "for classname in classnames:\n",
    "    idxset_tr = newtrgroupsu_idxset_of_trgroups[classname] \n",
    "    idxset_va = newvagroupsu_idxset_of_trgroups[classname]\n",
    "    newtrclips.extend(trclips_dc[classname][:len(idxset_tr)]) # trclips_dc (and trclips) includes both tr and va clips\n",
    "    newvaclips.extend(trclips_dc[classname][len(idxset_tr):]) # trclips_dc (and trclips) includes both tr and va clips\n",
    "\n",
    "\n",
    "assert 0 < len(newtrclips) < len(trclips)\n",
    "assert 0 < len(newvaclips) < len(trclips)\n",
    "assert len(newvaclips) + len(newtrclips) == len(trclips), \"Contamination (tr & va) detected!\"\n",
    "for v in newvaclips:\n",
    "    assert v in trclips\n",
    "for v in newtrclips:\n",
    "    assert v in trclips\n",
    "assert len(trclips) == len(np.unique(trclips))\n",
    "    \n",
    "# e.g., \n",
    "# newtrclips = \n",
    "# ['/April_09_brush_hair_u_nm_np1_ba_goo_0',\n",
    "#  '/April_09_brush_hair_u_nm_np1_ba_goo_1',\n",
    "#  '/April_09_brush_hair_u_nm_np1_ba_goo_2',\n",
    "#  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ri_med_3',\n",
    "#  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_0',\n",
    "#  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_1',\n",
    "#  '/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_med_2',\n",
    "#  '/Blonde_being_brushed_brush_hair_f_nm_np2_ri_med_0',\n",
    "#  '/Blonde_being_brushed_brush_hair_u_cm_np2_ri_med_1',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/valid/testlist01.txt \n",
    "# E.g., \"DATADIR/HMDB51/labels/ucfTrainValidTestlist/validlist01.txt\"\n",
    "#################################################\n",
    "# Comment-outed for safety\n",
    "with open(newtrtxt, mode=\"w\") as f:\n",
    "    l = len(newtrclips)\n",
    "    for i, v in enumerate(newtrclips):\n",
    "        f.write(v + \" \\n\")\n",
    "\n",
    "with open(newvatxt, mode=\"w\") as f:\n",
    "    l = len(newvaclips)\n",
    "    for i, v in enumerate(newvaclips):\n",
    "        f.write(v + \" \\n\")\n",
    "\n",
    "with open(newtetxt, mode=\"w\") as f:\n",
    "    l = len(newteclips)\n",
    "    for i, v in enumerate(newteclips):\n",
    "        f.write(v + \" \\n\")\n",
    "        \n",
    "# Assert\n",
    "with open(newtrtxt, mode=\"r\") as f:\n",
    "    a = f.readlines()\n",
    "with open(newvatxt, mode=\"r\") as f:\n",
    "    b = f.readlines()\n",
    "with open(newtetxt, mode=\"r\") as f:\n",
    "    c = f.readlines()\n",
    "for i in a:\n",
    "    assert not i in b\n",
    "    assert not i in c\n",
    "for i in b:\n",
    "    assert not i in a\n",
    "    assert not i in c\n",
    "for i in c:\n",
    "    assert not i in a\n",
    "    assert not i in b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
